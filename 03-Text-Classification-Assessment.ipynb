{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation de la classification des textes\n",
    "Cette évaluation ressemble beaucoup au projet de classification de texte que nous venons de terminer, et le jeu de données est très similaire.\n",
    "\n",
    "Le jeu de données **moviereviews2.tsv** contient le texte de 6000 critiques de films. 3000 sont positives, 3000 sont négatives, et le texte a été prétraité sous la forme d'un fichier délimité par des tabulations. Comme précédemment, les étiquettes sont données comme `pos` et `neg`.\n",
    "\n",
    "Nous avons inclus 20 critiques qui contiennent soit des données `NaN`, soit des chaînes composées d'espaces blancs.\n",
    "\n",
    "Pour plus d'informations sur cet ensemble de données, visitez http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #1 : Effectuer les importations et charger le jeu de données dans un DataFrame pandas\n",
    "Pour cet exercice, vous pouvez charger le jeu de données à partir de `'./moviereviews2.tsv'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>I loved this movie and will watch it again. Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos</td>\n",
       "      <td>A warm, touching movie that has a fantasy-like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>I was not expecting the powerful filmmaking ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>This so-called \"documentary\" tries to tell tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>This show has been my escape from reality for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   pos  I loved this movie and will watch it again. Or...\n",
       "1   pos  A warm, touching movie that has a fantasy-like...\n",
       "2   pos  I was not expecting the powerful filmmaking ex...\n",
       "3   neg  This so-called \"documentary\" tries to tell tha...\n",
       "4   pos  This show has been my escape from reality for ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('../data/TextFiles/moviereviews2.tsv', sep='\\t')\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #2 : Vérifier les valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "# Check for whitespace strings (it's OK if there aren't any!):\n",
    "blanks = []\n",
    "\n",
    "for i,lb,rv in df.itertuples():\n",
    "    if type(rv) == str:\n",
    "        if rv.isspace():\n",
    "            blanks.append(i)\n",
    "print(len(blanks), 'blanks: ', blanks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #3 : Supprimer les valeurs NaN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5980"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #4 : Jetez un coup d'œil rapide à la colonne `label` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    2990\n",
       "neg    2990\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #5 : Diviser les données en ensembles d'entrainement et de test :\n",
    "Vous pouvez utiliser les paramètres de votre choix. Pour comparer vos résultats à ma resolution, utilisez `test_size=0.33, random_state=42`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #6 : Construire un pipeline pour vectoriser la date, puis entraîner et ajuster un modèle\n",
    "Vous pouvez utiliser le modèle de votre choix. Pour comparer vos résultats à ma résolution, utilisez `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf_SVC = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', LinearSVC())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tâche #7 : Exécuter les prédictions et analyser les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "text_clf_nb.fit(X_train, y_train)\n",
    "predictions = text_clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[940  51]\n",
      " [136 847]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.95      0.91       991\n",
      "         pos       0.94      0.86      0.90       983\n",
      "\n",
      "    accuracy                           0.91      1974\n",
      "   macro avg       0.91      0.91      0.91      1974\n",
      "weighted avg       0.91      0.91      0.91      1974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9052684903748733\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
