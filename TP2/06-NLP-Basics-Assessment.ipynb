{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 ère partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL to perform standard imports:\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Créer un objet Doc à partir du fichier `owlcreek.txt`**<br>\n",
    "> HINT: Use `with open('../TextFiles/owlcreek.txt') as f:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here:\n",
    "with open(\"../../data/TextFiles/owlcreek.txt\", encoding=\"unicode-escape\") as f:\n",
    "    doc = nlp(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
       "\n",
       "by Ambrose Bierce\n",
       "\n",
       "I\n",
       "\n",
       "A man stood upon a railroad bridge in northern Alabama, looking down\n",
       "into the swift water twenty feet below.  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to verify it worked:\n",
    "\n",
    "doc[:36]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Combien de tokens sont contenus dans le fichier ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4835"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Combien de phrases sont contenues dans le fichier ?<br>HINT: Vous devez d'abord créer une liste !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence =[]\n",
    "for sent in doc.sents:\n",
    "    sentence.append(sent)\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Imprimer la deuxième phrase du document**<br> HINT: L'indexation commence à zéro et le titre compte comme la première phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The man's hands were behind\n",
       "his back, the wrists bound with a cord.  "
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pour chaque élément de la phrase ci-dessus, imprimez son`text`, `POS` tag, `dep` tag and `lemma`<br>\n",
    "CHALLENGE: Faire en sorte que les valeurs s'alignent en colonnes dans la sortie imprimée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The          DET    det    the\n",
      "man          NOUN   poss   man\n",
      "'s           PART   case   's\n",
      "hands        NOUN   nsubj  hand\n",
      "were         AUX    ROOT   be\n",
      "behind       ADP    prep   behind\n",
      "\n",
      "            SPACE  dep    \n",
      "\n",
      "his          PRON   poss   his\n",
      "back         NOUN   pobj   back\n",
      ",            PUNCT  punct  ,\n",
      "the          DET    det    the\n",
      "wrists       NOUN   appos  wrist\n",
      "bound        VERB   acl    bind\n",
      "with         ADP    prep   with\n",
      "a            DET    det    a\n",
      "cord         NOUN   pobj   cord\n",
      ".            PUNCT  punct  .\n",
      "             SPACE  dep     \n"
     ]
    }
   ],
   "source": [
    "# NORMAL SOLUTION:\n",
    "for token in sentence[1]:\n",
    "    print(f\"{token.text:{12}} {token.pos_:{6}} {token.dep_:{6}} {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHALLENGE SOLUTION:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 ème partie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1: Tokenisation de phrases avec SpaCy\n",
    "\n",
    "Question : Créez une phrase complexe en anglais et utilisez SpaCy pour la tokeniser en phrases individuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystring = \"I add so many memories with my grandparents but long story short, I don't talk to them anymore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Utilisez SpaCy pour tokeniser la phrase complexe en mots.\n",
    "\n",
    "Question : Quels avantages offre SpaCy par rapport à d'autres bibliothèques pour la tokenisation?\n",
    "\n",
    "Question : Créez une fonction custom_spacy_tokenizer qui prend une phrase en entrée, utilise SpaCy pour la tokeniser et renvoie les tokens en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_tokenizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I | add | so | many | memories | with | my | grandparents | but | long | story | short | , | I | do | n't | talk | to | them | anymore | "
     ]
    }
   ],
   "source": [
    "# Tokenisation en mots avec SpaCy\n",
    "doc = nlp(mystring)\n",
    "for token in doc:\n",
    "    print(token.text, end=' | ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'add',\n",
       " 'so',\n",
       " 'many',\n",
       " 'memories',\n",
       " 'with',\n",
       " 'my',\n",
       " 'grandparents',\n",
       " 'but',\n",
       " 'long',\n",
       " 'story',\n",
       " 'short',\n",
       " ',',\n",
       " 'I',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'talk',\n",
       " 'to',\n",
       " 'them',\n",
       " 'anymore']"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fonction de tokenisation personnalisée avec SpaCy\n",
    "custom_spacy_tokenizer(mystring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 3\n",
    "\n",
    "Question : Utilisez SpaCy pour extraire les lemmes et les POS de la phrase complexe.\n",
    "\n",
    "Question : Comment SpaCy gère-t-il la tokenisation des entités nommées? Testez cela sur une phrase contenant une entité nommée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Apple\n",
      "a major technology company\n"
     ]
    }
   ],
   "source": [
    "# Tokenisation des entités nommées avec SpaCy\n",
    "text_with_entity = \"Apple is a major technology company.\"\n",
    "for chunk in nlp(text_with_entity).ents:\n",
    "    print(chunk.text)\n",
    "\n",
    "for chunk in nlp(text_with_entity).noun_chunks:\n",
    "    print(chunk.text)\n",
    "\n",
    "# spaCy gère les entités nommées en retirant tout les déterminants qui ne sert à rien\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer le stemming sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming---->stem\n",
      "is---->is\n",
      "an---->an\n",
      "essential---->essenti\n",
      "part---->part\n",
      "of---->of\n",
      "natural---->natur\n",
      "language---->languag\n",
      "processing.---->processing.\n",
      "It---->it\n",
      "involves---->involv\n",
      "reducing---->reduc\n",
      "words---->word\n",
      "to---->to\n",
      "their---->their\n",
      "base---->base\n",
      "form.---->form.\n"
     ]
    }
   ],
   "source": [
    "# Création d'une phrase complexe\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "text = \"Stemming is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "for word in text.split():\n",
    "    print(word+ '---->' +p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_stemmer qui prend une phrase en entrée, utilise SpaCy pour effectuer le stemming, et renvoie les stems en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_stemmer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de stemming personnalisée avec SpaCy\n",
    "def custom_spacy_stemmer(sentence):\n",
    "    for word in sentence.split():\n",
    "        print(word+ '---->' +p_stemmer.stem(word))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming---->stem\n",
      "is---->is\n",
      "an---->an\n",
      "essential---->essenti\n",
      "part---->part\n",
      "of---->of\n",
      "natural---->natur\n",
      "language---->languag\n",
      "processing.---->processing.\n",
      "It---->it\n",
      "involves---->involv\n",
      "reducing---->reduc\n",
      "words---->word\n",
      "to---->to\n",
      "their---->their\n",
      "base---->base\n",
      "form.---->form.\n"
     ]
    }
   ],
   "source": [
    "custom_spacy_stemmer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1 \n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour effectuer la lemmatisation sur les mots de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization ---> lemmatization\n",
      "is ---> be\n",
      "an ---> an\n",
      "essential ---> essential\n",
      "part ---> part\n",
      "of ---> of\n",
      "natural ---> natural\n",
      "language ---> language\n",
      "processing ---> processing\n",
      ". ---> .\n",
      "It ---> it\n",
      "involves ---> involve\n",
      "reducing ---> reduce\n",
      "words ---> word\n",
      "to ---> to\n",
      "their ---> their\n",
      "base ---> base\n",
      "form ---> form\n",
      ". ---> .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# # Téléchargement du modèle de langue anglaise\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Lemmatization is an essential part of natural language processing. It involves reducing words to their base form.\"\n",
    "for token in nlp(text):\n",
    "    print(token.text, '--->', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction custom_spacy_lemmatizer qui prend une phrase en entrée, utilise SpaCy pour effectuer la lemmatisation, et renvoie les lemmes en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction custom_spacy_lemmatizer à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de lemmatisation personnalisée avec SpaCy\n",
    "def custom_spacy_lemmatizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, '--->', token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization ---> lemmatization\n",
      "is ---> be\n",
      "an ---> an\n",
      "essential ---> essential\n",
      "part ---> part\n",
      "of ---> of\n",
      "natural ---> natural\n",
      "language ---> language\n",
      "processing ---> processing\n",
      ". ---> .\n",
      "It ---> it\n",
      "involves ---> involve\n",
      "reducing ---> reduce\n",
      "words ---> word\n",
      "to ---> to\n",
      "their ---> their\n",
      "base ---> base\n",
      "form ---> form\n",
      ". ---> .\n"
     ]
    }
   ],
   "source": [
    "custom_spacy_lemmatizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 1\n",
    "\n",
    "Question : Créez une phrase complexe en anglais.\n",
    "\n",
    "Question : Utilisez SpaCy pour filtrer les stopwords de la phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords\n",
      "common\n",
      "words\n",
      "removed\n",
      "text\n",
      "preprocessing\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Téléchargement du modèle de langue anglaise\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Création d'une phrase complexe\n",
    "text = \"Stopwords are common words that are often removed during text preprocessing.\"\n",
    "for token in nlp(text):\n",
    "    if not token.is_stop:\n",
    "        print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercice 2\n",
    "\n",
    "Question : Créez une fonction remove_stopwords qui prend une phrase en entrée, utilise SpaCy pour filtrer les stopwords, et renvoie les mots restants en minuscules.\n",
    "\n",
    "Question : Appliquez votre fonction remove_stopwords à une phrase de votre choix et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de filtrage des stopwords avec SpaCy\n",
    "def remove_stopwords(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.text for token in doc if not token.is_stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stopwords', 'common', 'words', 'removed', 'text', 'preprocessing', '.']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codez une fonction 'preprocess_text_spacy' qui applique les méthodes de preprocessing vues en cours et permet d'obtenir les résultats prétraités suivants :\n",
    "\n",
    "NB : n'oublier pas de traiter les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    pass\n",
    "    nlp = spacy.load('en_core_web_sm')  \n",
    "    texts = nlp(text)\n",
    "    preprocessed_text = [re.sub(r'[^a-zA-Z]', '', token.text.lower()) for token in texts if not token.is_stop]\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " '',\n",
       " 'know',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'fascinating',\n",
       " 'field',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple d'utilisation avec SpaCy\n",
    "raw_text = \"Hello, I know that Natural Language Processing is a fascinating field!!!\"\n",
    "preprocess_text_spacy(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
